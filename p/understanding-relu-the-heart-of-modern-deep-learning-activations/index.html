<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="ReLU (Rectified Linear Unit) is a popular activation function in deep learning, known for its simplicity and effectiveness. By outputting zero for negative values and passing positive values unchanged, it helps networks learn faster and avoid the vanishing gradient problem. This blog explores how ReLU works, its advantages, and its common variants like Leaky ReLU and ELU, addressing challenges such as dying neurons in deep networks."><title>Understanding ReLU The Heart of Modern Deep Learning Activations</title>
<link rel=canonical href=https://shivvamm.github.io/p/understanding-relu-the-heart-of-modern-deep-learning-activations/><link rel=stylesheet href=/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="Understanding ReLU The Heart of Modern Deep Learning Activations"><meta property='og:description' content="ReLU (Rectified Linear Unit) is a popular activation function in deep learning, known for its simplicity and effectiveness. By outputting zero for negative values and passing positive values unchanged, it helps networks learn faster and avoid the vanishing gradient problem. This blog explores how ReLU works, its advantages, and its common variants like Leaky ReLU and ELU, addressing challenges such as dying neurons in deep networks."><meta property='og:url' content='https://shivvamm.github.io/p/understanding-relu-the-heart-of-modern-deep-learning-activations/'><meta property='og:site_name' content='Shivam Pandey'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='ReLU'><meta property='article:tag' content='Activation Functions'><meta property='article:tag' content='Deep Learning Algorithms'><meta property='article:tag' content='Neural Network Optimization'><meta property='article:tag' content='Machine Learning Basics'><meta property='article:tag' content='Leaky ReLU'><meta property='article:tag' content='ELU'><meta property='article:tag' content='Dying ReLU Problem'><meta property='article:tag' content='Artificial Intelligence Techniques'><meta property='article:tag' content='Convolutional Neural Networks'><meta property='article:published_time' content='2022-03-06T00:00:00+00:00'><meta property='article:modified_time' content='2022-03-06T00:00:00+00:00'><meta property='og:image' content='https://shivvamm.github.io/p/understanding-relu-the-heart-of-modern-deep-learning-activations/cover.png'><meta name=twitter:title content="Understanding ReLU The Heart of Modern Deep Learning Activations"><meta name=twitter:description content="ReLU (Rectified Linear Unit) is a popular activation function in deep learning, known for its simplicity and effectiveness. By outputting zero for negative values and passing positive values unchanged, it helps networks learn faster and avoid the vanishing gradient problem. This blog explores how ReLU works, its advantages, and its common variants like Leaky ReLU and ELU, addressing challenges such as dying neurons in deep networks."><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://shivvamm.github.io/p/understanding-relu-the-heart-of-modern-deep-learning-activations/cover.png'><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu7486436934494440064.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>üç•</span></figure><div class=site-meta><h1 class=site-name><a href=/>Shivam Pandey</a></h1><h2 class=site-description>AI/ML Engineer exploring how technology changes our lives. I dig deep into the digital world with a touch of humor, believing that understanding tech better makes the future clearer and a lot more enjoyable.</h2></div></header><ol class=menu-social><li><a href=https://github.com/shivvamm target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://twitter.com/Shivv71 target=_blank title=Twitter rel=me><svg class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><ol><li><a href=#relu-rectified-linear-unit-overview>ReLU (Rectified Linear Unit) Overview:</a></li><li><a href=#definition>Definition:</a></li><li><a href=#visual-representation>Visual Representation:</a></li><li><a href=#key-features-of-relu>Key Features of ReLU:</a></li><li><a href=#advantages-of-relu>Advantages of ReLU:</a></li><li><a href=#limitations-of-relu>Limitations of ReLU:</a></li><li><a href=#variants-of-relu>Variants of ReLU:</a></li><li><a href=#applications-of-relu>Applications of ReLU:</a></li><li><a href=#summary>Summary:</a></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/understanding-relu-the-heart-of-modern-deep-learning-activations/><img src=/p/understanding-relu-the-heart-of-modern-deep-learning-activations/cover_hu11281005765698789414.png srcset="/p/understanding-relu-the-heart-of-modern-deep-learning-activations/cover_hu11281005765698789414.png 800w, /p/understanding-relu-the-heart-of-modern-deep-learning-activations/cover_hu10607832019445579496.png 1600w" width=800 height=800 loading=lazy alt="Featured image of post Understanding ReLU The Heart of Modern Deep Learning Activations"></a></div><div class=article-details><header class=article-category><a href=/categories/deep-learning/>Deep Learning
</a><a href=/categories/neural-networks/>Neural Networks
</a><a href=/categories/machine-learning/>Machine Learning
</a><a href=/categories/artificial-intelligence/>Artificial Intelligence
</a><a href=/categories/data-science/>Data Science</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/understanding-relu-the-heart-of-modern-deep-learning-activations/>Understanding ReLU The Heart of Modern Deep Learning Activations</a></h2><h3 class=article-subtitle>ReLU (Rectified Linear Unit) is a popular activation function in deep learning, known for its simplicity and effectiveness. By outputting zero for negative values and passing positive values unchanged, it helps networks learn faster and avoid the vanishing gradient problem. This blog explores how ReLU works, its advantages, and its common variants like Leaky ReLU and ELU, addressing challenges such as dying neurons in deep networks.</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Mar 06, 2022</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>4 minute read</time></div></footer></div></header><section class=article-content><h3 id=relu-rectified-linear-unit-overview>ReLU (Rectified Linear Unit) Overview:</h3><p>ReLU stands for <strong>Rectified Linear Unit</strong>, and it is one of the most widely used activation functions in deep learning, especially in Convolutional Neural Networks (CNNs) and other neural networks. Its simplicity and effectiveness make it a popular choice for training deep learning models.</p><h3 id=definition>Definition:</h3><p>The ReLU function is mathematically defined as:</p>\[
f(x) = \max(0, x)
\]<p>In simple terms, it outputs:</p><ul><li><strong>x</strong> if x is positive,</li><li><strong>0</strong> if x is negative.</li></ul><p>This means that for any positive input, ReLU will return the input value, while for any negative input, it will return 0.</p><h3 id=visual-representation>Visual Representation:</h3><p>A plot of the ReLU function would show:</p><ul><li>A line with slope 1 for positive values of \( x \),</li><li>A flat line at 0 for negative values of \( x \).</li></ul><p>The graph looks like this:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>  |
</span></span><span class=line><span class=cl>  |         
</span></span><span class=line><span class=cl>  |        /
</span></span><span class=line><span class=cl>  |       /
</span></span><span class=line><span class=cl>  |      /
</span></span><span class=line><span class=cl>  |     /  
</span></span><span class=line><span class=cl>  |    /
</span></span><span class=line><span class=cl>  |___/__________
</span></span><span class=line><span class=cl>         0
</span></span></code></pre></td></tr></table></div></div><h3 id=key-features-of-relu>Key Features of ReLU:</h3><ol><li><p><strong>Non-linearity</strong>: Although it looks like a simple linear function, ReLU introduces non-linearity because the output for negative values is clamped to zero. This allows neural networks to model complex patterns and relationships in the data.</p></li><li><p><strong>Sparsity</strong>: ReLU introduces sparsity by setting all negative activations to zero. This makes the network &ldquo;sparse&rdquo; because many neurons may not activate for a given input. Sparsity can help in faster training and better generalization.</p></li><li><p><strong>Computational Efficiency</strong>: ReLU is very computationally efficient because it only involves simple thresholding (i.e., comparing with 0), which is much less computationally expensive than some other activation functions like sigmoid or tanh.</p></li></ol><h3 id=advantages-of-relu>Advantages of ReLU:</h3><ol><li><p><strong>Faster convergence</strong>: ReLU can significantly speed up the training process compared to sigmoid or tanh because it doesn‚Äôt saturate (especially for large positive values), which helps in gradient-based optimization.</p></li><li><p><strong>Gradient flow</strong>: ReLU avoids the vanishing gradient problem that occurs in functions like the sigmoid or tanh. These activation functions tend to squash large values into a small range, causing very small gradients during backpropagation. In ReLU, the gradient is either 1 (for positive inputs) or 0 (for negative inputs), making it more effective for training deeper networks.</p></li><li><p><strong>Sparsity and efficient representation</strong>: Because of the zeroing out of negative values, ReLU-based networks tend to have sparse activations, which can lead to better efficiency and potentially better performance, especially for large-scale models.</p></li></ol><h3 id=limitations-of-relu>Limitations of ReLU:</h3><ol><li><p><strong>Dying ReLU problem</strong>: One issue with ReLU is that for negative input values, the function always outputs 0, which can cause neurons to &ldquo;die&rdquo; during training. This means the neurons stop learning because their gradients are zero, and they never activate. This is especially problematic when using large learning rates or initializing weights poorly.</p></li><li><p><strong>Not bounded</strong>: ReLU is unbounded for positive inputs (i.e., there is no upper limit), which can sometimes lead to numerical instability or exploding gradients, especially for deep networks.</p></li></ol><h3 id=variants-of-relu>Variants of ReLU:</h3><p>To address some of the issues with standard ReLU, several variants have been proposed:</p><ol><li><p><strong>Leaky ReLU</strong>:</p><ul><li>In Leaky ReLU, instead of outputting 0 for negative values, a small slope (like 0.01) is used. Mathematically, it is defined as:
\[
f(x) = \begin{cases}
x & \text{if } x > 0 \\
     \alpha x & \text{if } x \leq 0
     \end{cases}
\]
where \( \alpha \) is a small constant (usually a value like 0.01). This helps avoid the dying ReLU problem.</li></ul></li><li><p><strong>Parametric ReLU (PReLU)</strong>:</p><ul><li>Similar to Leaky ReLU, but here \( \alpha \) is learned during training, meaning it is a parameter of the model.</li></ul></li><li><p><strong>Exponential Linear Unit (ELU)</strong>:</p><ul><li>ELU is another variant where the function for negative values is:
\[
f(x) = \begin{cases}
x & \text{if } x > 0 \\
     \alpha (e^x - 1) & \text{if } x \leq 0
     \end{cases}
\]
where \( \alpha \) is a constant. The ELU can help with the issue of dead neurons while also having smoother gradients for negative inputs.</li></ul></li><li><p><strong>Scaled Exponential Linear Unit (SELU)</strong>:</p><ul><li>SELU is a self-normalizing variant of ELU designed to ensure that activations in a network have mean 0 and variance 1, helping the network learn efficiently without explicit batch normalization.</li></ul></li></ol><h3 id=applications-of-relu>Applications of ReLU:</h3><ul><li><p><strong>Convolutional Neural Networks (CNNs)</strong>: ReLU is extensively used in CNNs for tasks like image classification, object detection, and more, as it works well with the high-dimensional data involved.</p></li><li><p><strong>Fully Connected Networks</strong>: It‚Äôs also widely used in deep fully connected networks for tasks like speech recognition, natural language processing, and more.</p></li><li><p><strong>Reinforcement Learning</strong>: ReLU is often used in deep reinforcement learning models for decision-making problems.</p></li></ul><h3 id=summary>Summary:</h3><ul><li><strong>ReLU</strong> is a simple, fast, and effective activation function.</li><li>It helps avoid the vanishing gradient problem, leading to faster and more stable training.</li><li>Variants like <strong>Leaky ReLU</strong> and <strong>ELU</strong> address the problem of dying neurons and can be useful in certain scenarios.</li><li>Despite its simplicity, ReLU has proven to be highly effective in deep learning and has become the default activation function in many modern neural networks.</li></ul><blockquote><p>Generated by AI</p></blockquote></section><footer class=article-footer><section class=article-tags><a href=/tags/relu/>ReLU</a>
<a href=/tags/activation-functions/>Activation Functions</a>
<a href=/tags/deep-learning-algorithms/>Deep Learning Algorithms</a>
<a href=/tags/neural-network-optimization/>Neural Network Optimization</a>
<a href=/tags/machine-learning-basics/>Machine Learning Basics</a>
<a href=/tags/leaky-relu/>Leaky ReLU</a>
<a href=/tags/elu/>ELU</a>
<a href=/tags/dying-relu-problem/>Dying ReLU Problem</a>
<a href=/tags/artificial-intelligence-techniques/>Artificial Intelligence Techniques</a>
<a href=/tags/convolutional-neural-networks/>Convolutional Neural Networks</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Last updated on Mar 06, 2022 00:00 UTC</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/topplatforms-for-ai-ml-engineers-to-showcase-skills-and-compete/><div class=article-image><img src=/p/topplatforms-for-ai-ml-engineers-to-showcase-skills-and-compete/cover.32407e64d88042ed978320d7bc16b9df_hu1054387559256386189.jpeg width=250 height=150 loading=lazy alt="Featured image of post Top Platforms for AI/ML Engineers to Showcase Skills and Compete" data-key=TopPlatforms-for-AI-ML-Engineers-to-Showcase-Skills-and-Compete data-hash="md5-MkB+ZNiAQu2XgyDXvBa53w=="></div><div class=article-details><h2 class=article-title>Top Platforms for AI/ML Engineers to Showcase Skills and Compete</h2></div></a></article></div></div></aside><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//hugo-theme-stack.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 Shivam Pandey</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.29.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>