[{"content":"These are four popular approximate nearest neighbor (ANN) search architectures, designed to efficiently search through large datasets, especially in high-dimensional spaces. Each has its own strengths, weaknesses, and use cases. Let me explain them one by one:\n1. Faiss (Facebook AI Similarity Search) Overview: Faiss is an open-source library developed by Facebook AI Research (FAIR) designed for efficient similarity search and clustering of dense vectors. It is widely used in machine learning and AI applications, such as recommendation systems, image retrieval, and natural language processing. Key Features: Exact and Approximate Search: Faiss supports both exact nearest neighbor (NN) search and approximate search using various techniques, such as quantization and indexing. Indexing Methods: Faiss includes several indexing methods to optimize the search: Flat (Brute-Force Search): Exhaustively compares all vectors. It’s the slowest but guarantees accuracy. Product Quantization (PQ): A lossy compression method that reduces memory usage and speeds up search by approximating the distances. IVF (Inverted File): An indexing method based on partitioning the space into clusters (or cells) and searching within the relevant cluster. HNSW (Hierarchical Navigable Small World): A graph-based search method for fast and approximate nearest neighbor retrieval. GPU Support: Faiss can also leverage GPUs for faster computations, making it highly scalable for large datasets. Use Cases: Image and video search, semantic search in NLP, and large-scale recommendation systems. 2. HNSW (Hierarchical Navigable Small World) Overview: HNSW is an approximate nearest neighbor search algorithm based on the construction of small-world graphs. It aims to provide fast nearest neighbor search while maintaining a balance between search accuracy and speed. Key Features: Graph Construction: The HNSW algorithm builds a multi-layer graph, where each layer is a subset of the data points. The graph is navigable, meaning you can efficiently explore neighboring nodes to find nearest neighbors. Navigability: The algorithm uses small-world properties, where most nodes are close to each other, allowing for fast traversal. Performance: It provides high recall (accuracy) at relatively low computational cost. The performance improves with the number of layers in the graph. Memory Efficiency: HNSW has a relatively high memory overhead due to the need to store the graph structure, but it is highly efficient in terms of query speed. Use Cases: Large-scale nearest neighbor search, such as in image retrieval, NLP, and recommendation engines. 3. DiskANN Overview: DiskANN (Disk-based Approximate Nearest Neighbor) is a variant of the ANN search algorithms designed to handle extremely large datasets that do not fit into memory by utilizing disk storage efficiently. Key Features: Disk-Backed Search: Unlike traditional memory-based search algorithms, DiskANN is designed to perform ANN search while storing the dataset on disk rather than in memory. It uses efficient I/O techniques to minimize disk read operations. Scalability: DiskANN is suitable for very large datasets (terabytes or more) and works by compressing data and storing it in a way that minimizes disk access during search. Approximation: As with other ANN algorithms, DiskANN sacrifices some search accuracy in exchange for speed. It uses indexing techniques to accelerate search without needing to scan every point on disk. Hybrid Models: The algorithm may combine multiple indexing structures (e.g., HNSW with disk-based techniques) to balance search performance and memory usage. Use Cases: Used in scenarios where data is too large to fit into memory, such as large-scale image and video search, genomic data analysis, and recommendation systems for big data. 4. ScaNN (Scalable Nearest Neighbor) Overview: ScaNN is a fast and efficient nearest neighbor search algorithm developed by Google Research, designed to handle high-dimensional datasets. It uses advanced techniques like quantization and hashing to speed up the search process while maintaining accuracy. Key Features: Quantization: ScaNN applies quantization techniques (such as vector quantization) to compress the dataset and reduce memory usage. This allows for faster searches at the cost of some approximation. Hybrid Search: ScaNN uses a combination of multiple search methods, including tree-based and graph-based search, to balance query time and recall. It also incorporates inverted indexing for fast lookups. Efficient Search: ScaNN offers state-of-the-art speed and accuracy for high-dimensional vector search by applying a combination of filtering and dimensionality reduction techniques. Performance Tuning: ScaNN allows users to fine-tune the balance between speed and accuracy by adjusting several hyperparameters. Use Cases: Large-scale machine learning models (e.g., search engines, image retrieval, recommendation systems), NLP models like BERT, and similarity search on high-dimensional embeddings. Comparison of Key Characteristics: Feature Faiss HNSW DiskANN ScaNN Accuracy High (with exact search) High (approximate) Moderate (approximate) High (adjustable) Search Speed Fast with indexing Very fast (graph-based) Fast (disk-based) Very fast (quantized) Memory Usage Memory-efficient with PQ High due to graph storage Efficient disk usage Memory-efficient Scalability High (GPU support) High (graph scales) Very high (disk-based) High (fine-tuning) Use Case NLP, image search Large-scale ANN search Large-scale disk-based search ML models, embeddings Summary: Faiss is versatile and widely used in machine learning for both exact and approximate nearest neighbor search, with strong support for GPU acceleration. HNSW is excellent for high-speed approximate search, especially with its small-world graph structure, suitable for general-purpose large-scale ANN search. DiskANN is specifically designed for large datasets that don\u0026rsquo;t fit in memory, making it suitable for disk-backed ANN searches at scale. ScaNN offers an efficient, scalable solution for large datasets, focusing on fast search times and flexibility in tuning accuracy vs. speed. Each of these architectures can be adapted to different scenarios depending on the trade-offs between accuracy, speed, memory usage, and scalability.\nGenerated by AI\n","date":"2024-11-28T00:00:00Z","image":"https://shivvamm.github.io/p/ann-search-architectures/cover_hu6307248181568134095.jpg","permalink":"https://shivvamm.github.io/p/ann-search-architectures/","title":"ANN Search Architectures"},{"content":"For AI/ML engineers, there are several platforms where they can compete, showcase their skills, and gain recognition in the field. Here are some popular platforms that focus on AI and ML challenges:\n1. Kaggle Overview: Kaggle is one of the most popular platforms for data science, machine learning, and AI challenges. It offers a variety of datasets, competitions, and notebooks that allow users to solve real-world problems and demonstrate their skills. Competitions: Kaggle hosts competitions where participants can work on problems related to data analysis, predictive modeling, and machine learning. Some competitions have substantial cash prizes and are sponsored by large companies. Learning: Kaggle also provides learning resources such as tutorials and courses to help individuals improve their AI/ML knowledge. Community: Kaggle has a large, active community that shares solutions, approaches, and discussions. Website: https://www.kaggle.com/\n2. DrivenData Overview: Similar to Kaggle, DrivenData offers data science and machine learning competitions but with a focus on social impact and non-profit organizations. It challenges participants to solve problems in areas such as public health, education, and conservation. Competitions: Competitions are geared toward practical, real-world problems, and solutions often lead to measurable impacts for organizations. Community: DrivenData also has a collaborative community of data scientists and AI professionals. Website: https://www.drivendata.org/\n3. Topcoder Overview: Topcoder is a global competitive platform for various domains, including AI and ML. It offers crowdsourcing competitions in a wide range of technical areas. Competitions: Topcoder hosts algorithm challenges, data science challenges, and AI/ML competitions where participants can test their skills against a global pool of talent. Opportunities: The platform offers not only competitions but also opportunities for paid freelance work with clients in need of AI/ML expertise. Website: https://www.topcoder.com/\n4. Zindi Overview: Zindi is a platform dedicated to solving Africa\u0026rsquo;s most pressing challenges using data science and AI. Zindi hosts AI/ML competitions that solve problems across a variety of sectors, including agriculture, health, and energy. Competitions: These competitions often involve building predictive models, working with large datasets, and providing innovative solutions to real-world issues. Community: Zindi is known for its strong focus on the African continent but has a global community of data scientists and AI professionals. Website: https://zindi.africa/\n5. AIcrowd Overview: AIcrowd is a platform that hosts AI and machine learning challenges, including reinforcement learning, natural language processing, and more. Competitions: AIcrowd hosts both theoretical and practical challenges, allowing participants to apply cutting-edge AI methods to real-world problems. Community: It features a collaborative environment where participants can share solutions and insights. Website: https://www.aicrowd.com/\n6. Codalab Overview: Codalab is a platform for running data science and machine learning competitions. It is often used by academic institutions and research organizations for organizing challenges in AI and ML. Competitions: Codalab supports a variety of AI/ML competitions, including computer vision, NLP, and predictive modeling challenges. Academic Focus: Codalab is widely used in research and educational settings for conducting AI-related challenges. Website: https://competitions.codalab.org/\n7. Hackerearth (AI/ML Challenges) Overview: While Hackerearth is known for its coding challenges, it also hosts AI and machine learning competitions, which allow participants to solve real-world ML problems. Competitions: Hackerearth offers challenges across various AI/ML domains, and participants can build and test models for real-world datasets. Opportunities: The platform also provides job opportunities and internship listings for those who excel in its challenges. Website: https://www.hackerearth.com/challenges/\n8. CodaLab Overview: CodaLab allows organizations to set up and run AI/ML competitions. It’s a popular platform for academic and industrial research challenges. Competitions: There are a variety of competitions related to data science, AI modeling, and real-world problem-solving. Community: It’s a great platform for collaboration, often utilized for open challenges and academic contests. Website: https://competitions.codalab.org/\n9. Machine Learning Competitions on GitHub Overview: GitHub hosts various repositories where AI/ML engineers can participate in open-source projects, collaborate with others, or contribute to ongoing competitions. Community: Engaging with open-source projects and competitions on GitHub helps engineers build a public portfolio of their work, which can be valuable for career growth. Website: https://github.com/\n10. AI Challenges on DevPost Overview: DevPost is a platform where developers can participate in hackathons, including AI/ML-focused events. Many organizations run competitions on DevPost to identify innovative AI solutions. Competitions: Challenges are often centered around building AI models or developing creative AI-based solutions for specific domains. Hackathons: It also includes hackathons that focus on AI and data science projects. Website: https://devpost.com/\n11. Fast.ai Competitions Overview: Fast.ai is a deep learning research group and platform that also hosts competitions and projects, especially focused on democratizing deep learning and AI. Competitions: Fast.ai often holds challenges that allow participants to apply cutting-edge deep learning techniques. Learning: It’s also a great platform to learn and experiment with state-of-the-art techniques in deep learning. Website: https://www.fast.ai/\n12. MLPerf Overview: MLPerf is a benchmarking suite for measuring the performance of machine learning hardware, software, and services. Competitions: MLPerf hosts competitions focused on improving performance in training and inference for a variety of ML tasks. Community: It\u0026rsquo;s a collaborative platform for researchers and engineers to work on improving AI/ML performance. Website: https://mlperf.org/\nThese platforms allow AI/ML engineers to not only compete but also learn, collaborate, and build portfolios that can be showcased to potential employers or collaborators in the field.\nGenerated by AI\n","date":"2022-11-28T00:00:00Z","image":"https://shivvamm.github.io/p/topplatforms-for-ai-ml-engineers-to-showcase-skills-and-compete/cover_hu18086236391645554193.jpeg","permalink":"https://shivvamm.github.io/p/topplatforms-for-ai-ml-engineers-to-showcase-skills-and-compete/","title":"Top Platforms for AI/ML Engineers to Showcase Skills and Compete"},{"content":"Understanding Cyclomatic Complexity: A Key Metric for Code Quality When it comes to writing and maintaining software, one of the biggest challenges developers face is managing the complexity of their code. This is where cyclomatic complexity comes into play—a metric that measures the logical complexity of a program by analyzing its control flow. It provides valuable insights into code quality, maintainability, and testing requirements.\nWhat is Cyclomatic Complexity? Cyclomatic complexity, introduced by Thomas J. McCabe in 1976, is a quantitative measure of the number of linearly independent paths through a program\u0026rsquo;s source code. Simply put, it evaluates how many decision points (e.g., if, while, for, switch) exist in your code and how these decisions affect its overall structure.\nWhy is Cyclomatic Complexity Important? This metric plays a crucial role in software development and testing for several reasons:\nCode Quality: Lower complexity often correlates with cleaner, more maintainable code. It helps developers pinpoint overly complicated functions or modules that may need refactoring.\nTesting: Cyclomatic complexity determines the minimum number of test cases required to achieve complete branch coverage. This ensures all possible execution paths are tested, leading to more robust software.\nMaintainability: High complexity increases the difficulty of understanding and modifying code. Simplifying complex sections improves maintainability and reduces the risk of introducing bugs.\nPredictability: Complex code often leads to higher defect rates. By managing complexity, teams can create more predictable and reliable software.\nHow is Cyclomatic Complexity Calculated? Cyclomatic complexity is calculated using a program\u0026rsquo;s control flow graph, where:\nNodes represent statements or blocks of code. Edges represent the flow of control between these statements. The formula is:\n\\[ V(G) = E - N + 2P \\]Where:\n\\( E \\): Number of edges \\( N \\): Number of nodes \\( P \\): Number of connected components (e.g., independent functions or modules) What Do Cyclomatic Complexity Numbers Mean? The complexity value helps assess the maintainability of code:\n1–10: Low complexity, easy to test and maintain. 11–20: Moderate complexity, requires more effort for testing and maintenance. 21–50: High complexity, testing and maintenance become challenging. \u0026gt;50: Very high complexity, indicates the need for immediate refactoring. Example: Measuring Complexity in Code Here’s an example in Python:\n1 2 3 4 5 6 7 def example(x): if x \u0026gt; 0: print(\u0026#34;Positive\u0026#34;) elif x \u0026lt; 0: print(\u0026#34;Negative\u0026#34;) else: print(\u0026#34;Zero\u0026#34;) For this function:\nNodes: 4 (Start, if, elif, else) Edges: 5 (Control flow between nodes) \\( P = 1 \\): Single function Using the formula:\n\\[ V(G) = E - N + 2P = 5 - 4 + 2 = 3 \\]The function has a cyclomatic complexity of 3, indicating there are 3 independent paths through the code.\nBest Practices to Manage Cyclomatic Complexity Keep Functions Small: Break down large functions into smaller, reusable components. Avoid Deep Nesting: Refactor heavily nested logic into simpler constructs. Use Early Returns: Minimize branching by returning early when conditions are met. Write Unit Tests: Ensure all paths through the code are adequately tested. Final Thoughts Cyclomatic complexity is more than just a number—it’s a powerful tool for developers to understand their code better. By keeping complexity in check, you can write more maintainable, testable, and reliable software. Whether you’re building a small project or a large-scale application, this metric should be part of your toolkit for crafting high-quality code.\nGenerated by AI\n","date":"2022-03-06T00:00:00Z","image":"https://shivvamm.github.io/p/cyclomatic-complexity/cover_hu13047096591486945632.jpg","permalink":"https://shivvamm.github.io/p/cyclomatic-complexity/","title":"Cyclomatic Complexity"},{"content":"Welcome to Hugo theme Stack. This is your first post. Edit or delete it, then start writing!\nFor more information about this theme, check the documentation: https://stack.jimmycai.com/\nWant a site like this? Check out hugo-theme-stack-stater\nsample text cvbc\nPhoto by Pawel Czerwinski on Unsplash\n","date":"2022-03-06T00:00:00Z","image":"https://shivvamm.github.io/p/hello-world/cover_hu6307248181568134095.jpg","permalink":"https://shivvamm.github.io/p/hello-world/","title":"Hello World"},{"content":"ReLU (Rectified Linear Unit) Overview: ReLU stands for Rectified Linear Unit, and it is one of the most widely used activation functions in deep learning, especially in Convolutional Neural Networks (CNNs) and other neural networks. Its simplicity and effectiveness make it a popular choice for training deep learning models.\nDefinition: The ReLU function is mathematically defined as:\n\\[ f(x) = \\max(0, x) \\]In simple terms, it outputs:\nx if x is positive, 0 if x is negative. This means that for any positive input, ReLU will return the input value, while for any negative input, it will return 0.\nVisual Representation: A plot of the ReLU function would show:\nA line with slope 1 for positive values of \\( x \\), A flat line at 0 for negative values of \\( x \\). The graph looks like this:\n1 2 3 4 5 6 7 8 9 | | | / | / | / | / | / |___/__________ 0 Key Features of ReLU: Non-linearity: Although it looks like a simple linear function, ReLU introduces non-linearity because the output for negative values is clamped to zero. This allows neural networks to model complex patterns and relationships in the data.\nSparsity: ReLU introduces sparsity by setting all negative activations to zero. This makes the network \u0026ldquo;sparse\u0026rdquo; because many neurons may not activate for a given input. Sparsity can help in faster training and better generalization.\nComputational Efficiency: ReLU is very computationally efficient because it only involves simple thresholding (i.e., comparing with 0), which is much less computationally expensive than some other activation functions like sigmoid or tanh.\nAdvantages of ReLU: Faster convergence: ReLU can significantly speed up the training process compared to sigmoid or tanh because it doesn’t saturate (especially for large positive values), which helps in gradient-based optimization.\nGradient flow: ReLU avoids the vanishing gradient problem that occurs in functions like the sigmoid or tanh. These activation functions tend to squash large values into a small range, causing very small gradients during backpropagation. In ReLU, the gradient is either 1 (for positive inputs) or 0 (for negative inputs), making it more effective for training deeper networks.\nSparsity and efficient representation: Because of the zeroing out of negative values, ReLU-based networks tend to have sparse activations, which can lead to better efficiency and potentially better performance, especially for large-scale models.\nLimitations of ReLU: Dying ReLU problem: One issue with ReLU is that for negative input values, the function always outputs 0, which can cause neurons to \u0026ldquo;die\u0026rdquo; during training. This means the neurons stop learning because their gradients are zero, and they never activate. This is especially problematic when using large learning rates or initializing weights poorly.\nNot bounded: ReLU is unbounded for positive inputs (i.e., there is no upper limit), which can sometimes lead to numerical instability or exploding gradients, especially for deep networks.\nVariants of ReLU: To address some of the issues with standard ReLU, several variants have been proposed:\nLeaky ReLU:\nIn Leaky ReLU, instead of outputting 0 for negative values, a small slope (like 0.01) is used. Mathematically, it is defined as: \\[ f(x) = \\begin{cases} x \u0026 \\text{if } x \u003e 0 \\\\ \\alpha x \u0026 \\text{if } x \\leq 0 \\end{cases} \\] where \\( \\alpha \\) is a small constant (usually a value like 0.01). This helps avoid the dying ReLU problem. Parametric ReLU (PReLU):\nSimilar to Leaky ReLU, but here \\( \\alpha \\) is learned during training, meaning it is a parameter of the model. Exponential Linear Unit (ELU):\nELU is another variant where the function for negative values is: \\[ f(x) = \\begin{cases} x \u0026 \\text{if } x \u003e 0 \\\\ \\alpha (e^x - 1) \u0026 \\text{if } x \\leq 0 \\end{cases} \\] where \\( \\alpha \\) is a constant. The ELU can help with the issue of dead neurons while also having smoother gradients for negative inputs. Scaled Exponential Linear Unit (SELU):\nSELU is a self-normalizing variant of ELU designed to ensure that activations in a network have mean 0 and variance 1, helping the network learn efficiently without explicit batch normalization. Applications of ReLU: Convolutional Neural Networks (CNNs): ReLU is extensively used in CNNs for tasks like image classification, object detection, and more, as it works well with the high-dimensional data involved.\nFully Connected Networks: It’s also widely used in deep fully connected networks for tasks like speech recognition, natural language processing, and more.\nReinforcement Learning: ReLU is often used in deep reinforcement learning models for decision-making problems.\nSummary: ReLU is a simple, fast, and effective activation function. It helps avoid the vanishing gradient problem, leading to faster and more stable training. Variants like Leaky ReLU and ELU address the problem of dying neurons and can be useful in certain scenarios. Despite its simplicity, ReLU has proven to be highly effective in deep learning and has become the default activation function in many modern neural networks. Generated by AI\n","date":"2022-03-06T00:00:00Z","image":"https://shivvamm.github.io/p/understanding-relu-the-heart-of-modern-deep-learning-activations/cover_hu8528636420115191206.png","permalink":"https://shivvamm.github.io/p/understanding-relu-the-heart-of-modern-deep-learning-activations/","title":"Understanding ReLU The Heart of Modern Deep Learning Activations"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code A B C D E F Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus ultricies, sapien non euismod aliquam, dui ligula tincidunt odio, at accumsan nulla sapien eget ex. Proin eleifend dictum ipsum, non euismod ipsum pulvinar et. Vivamus sollicitudin, quam in pulvinar aliquam, metus elit pretium purus Proin sit amet velit nec enim imperdiet vehicula. Ut bibendum vestibulum quam, eu egestas turpis gravida nec Sed scelerisque nec turpis vel viverra. Vivamus vitae pretium sapien Code Blocks Code block with backticks 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Diff code block 1 2 3 4 5 [dependencies.bevy] git = \u0026#34;https://github.com/bevyengine/bevy\u0026#34; rev = \u0026#34;11f52b8c72fc3a568e8bb4a4cd1f3eb025ac2e13\u0026#34; - features = [\u0026#34;dynamic\u0026#34;] + features = [\u0026#34;jpeg\u0026#34;, \u0026#34;dynamic\u0026#34;] One line code block 1 \u0026lt;p\u0026gt;A paragraph\u0026lt;/p\u0026gt; List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL + ALT + Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nThe above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2023-09-07T00:00:00Z","permalink":"https://shivvamm.github.io/p/markdown-syntax-guide/","title":"Markdown Syntax Guide"},{"content":"Hugo theme Stack supports the creation of interactive image galleries using Markdown. It\u0026rsquo;s powered by PhotoSwipe and its syntax was inspired by Typlog.\nTo use this feature, the image must be in the same directory as the Markdown file, as it uses Hugo\u0026rsquo;s page bundle feature to read the dimensions of the image. External images are not supported.\nSyntax 1 ![Image 1](1.jpg) ![Image 2](2.jpg) Result Photo by mymind and Luke Chesser on Unsplash\n","date":"2023-08-26T00:00:00Z","image":"https://shivvamm.github.io/p/image-gallery/2_hu15576070775610481867.jpg","permalink":"https://shivvamm.github.io/p/image-gallery/","title":"Image gallery"},{"content":"For more details, check out the documentation.\nBilibili video Tencent video YouTube video Generic video file Your browser doesn't support HTML5 video. Here is a link to the video instead. Gist GitLab Quote Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n― A famous person, The book they wrote Photo by Codioful on Unsplash\n","date":"2023-08-25T00:00:00Z","image":"https://shivvamm.github.io/p/shortcodes/cover_hu17063188895770243625.jpg","permalink":"https://shivvamm.github.io/p/shortcodes/","title":"Shortcodes"},{"content":"Stack has built-in support for math typesetting using KaTeX.\nIt\u0026rsquo;s not enabled by default side-wide, but you can enable it for individual posts by adding math: true to the front matter. Or you can enable it side-wide by adding math = true to the params.article section in config.toml.\nInline math This is an inline mathematical expression: $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$\n1 $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$ Block math $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$ 1 2 3 $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$ $$ f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi $$ 1 2 3 $$ f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi $$ ","date":"2023-08-24T00:00:00Z","permalink":"https://shivvamm.github.io/p/math-typesetting/","title":"Math Typesetting"}]